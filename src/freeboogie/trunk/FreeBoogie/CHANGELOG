This file contains a history of the design decisions that were
taken and their motivation.

Principle: Treat arrays as functions (NOT procedures) as much as possible.

We introduced generics in BoogiePL so that we can write
  var heap<x> : [ref,<x>name]x;
  procedure store<x,y,z>([x,y]z,x,y,z) returns ([x,y]z);
To specialize we write
  heap' := store<`ref,`<int>name,`int>(heap<`int>,obj,int_field,3);
Specialization is completely optional. The ` is used to keep
the parser simple. If omitted, then in
  call p(a<b,c<d,e<f,g<h)
the parser could figure out only at the very end that the '<'
signs mean "less than", not specialization. Even so, the grammar
uses an syntactic predicate for stuff like
  call a<`[[[int]int]int]int>, b, c := procThatReturnsTriple();
to look and see if there is an ':=' or not. The problem is that
you can have arbitrary long types before you see whether you are
calling a specialized function or simply specialize an ID. (The
existence of _any_ type constructor makes types arbitrary long.)


The construct
  var heap : [ref,<x>name]x;
is not legal anymore, but I plan to make it so in a 'compatibility'
mode. In 'compatibility mode' FB is supposed to do some extra
preprocessing of the source produced by Boogie to make it
legal FB code.


Arrays should be allowed to be used as indices within arrays.
This is similar to sending an array as an argument to a function.
(Of course, one could also argue that functions should be
good indices too by a reversed argument. And that would probably
be good but seems too hard for now.) A consequence is that in
VC generation we need to add `extensionality axioms' for arrays.
These are needed anyway if the user says
  assume a == b;
with a and b being arrays. 


I allow type variables in axioms so that it's possible to write
  function select<x,y>([x]y,x) returns (y);
  function update<x,y>([x]y,x,y) returns ([x]y);
  axiom<x,y> (forall a:[x]y,i:x,v:y:: select(update(a,i,v),i)==v);
The placement of the type variable introduction is consistent
if you imagine that axioms have an empty name. In fact I may
consider naming them in the future.


I must allow generics on all variable declarations. Otherwise I
can't express things like
  const i : <int>name;
  const v : int;
  axiom (forall h<x>:[<x>name]x :: h[i] == v);
Note that moving the <x> on the axiom does not typecheck.


The type system with generics is more expressive than the one
used by MS. Yet, the input needs to be adjusted in order to type-check.
The modifications on a typical file generated by Spec# fall in four
categories:
1. h : [ref,<x>name]x -->
   h<x> : [ref,<x>name]x
2. axiom (forall h:[ref,<x>name],f:name:: boo(h[*,f])) -->
   axiom<y> (forall h<x>:[ref,<x>name],f:name:: boo(h<`y>[*,f]))
3. ensures (forall f:name::old(Heap[*,f])==Heap[*,f]) -->
   ensures<x> (forall f:name::old(Heap<`x>[*,f])==Heap<`x>[*,f])
4. ensures Heap[*,A]==B -->
   ensures Heap<`name>[*,A]==B (because B : name)
Note that even with these changes the code still wouldn't type-check.
That is why in "old" mode the type-checking is relaxed so that
<S>T <: T and name <: <X>name, which seems unsafe, but... it's a
hack anyway. The more interesting part is that the four categories
listed can be collapsed into two categories by obeying these rules.
1. Try to fix name resolution by adding generics on the innermost
   variable declaration / specification / axiom / procedure / function.
2. Try to fix type-checking as follows.
   For each expression that fails typechecking because it should
   have a "real" type and yet the (bottom-up) type-checking comes
   up with an (unbound) type variable:
     a) Try to infer (top-down) the real types for all the type
        variables involved in type-checking the expression.
     b) For type variables that cannot be bound to real types
        add a fresh generic type to the axiom/spec/func.
     c) Add explicit specialization to all the identifiers
        (AtomId, AtomFun, etc.) occurring in the expression.
The step (2a) is not well defined, yet.


Implementation of the "old" mode will be done using a slightly more
general mechanism of "forgiving analysis". An analysis is a visitor
that produces a list of errors. A fixer is a transformer. A forgiving
analysis is obtained by combining an analysis with a fixer using
the following algorithm:
  Do
    errors = analyze(ast)
    ast = fix(ast, errors)
  While the number of errors decreases.
Notice that if 0 errors are returned we can stop right away since 0
cannot be decreased further. There will be a piece of code that glues
an analysis and a fixer to produce a forgiving analysis. The connection
between analysis and fixers will be loose: All they know in common is
what an "error" is. So, what is an error? An error has a type, an
associated AST node (location), perhaps additional information, 
and can print itself. (This can be implemented using subtyping or 
an enum and a printf-like language for constructing error messages 
from the additional information.)



vim:spell:fdm=manual:
